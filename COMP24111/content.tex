% Set the author and title of the compiled pdf
\hypersetup{
  pdftitle = {\Title},
  pdfauthor = {\Author}
}

\section{Machine Learning}

Machine Learning is the creation of self-configuring data structures that allow
a computer to do things that would be classed as `intelligent' if a human did
it.

Machine learning has been around in it's infancy since the 40's, where reasoning
and logic were first studied by Claude Shannon and Kurt Godel. Steady progress
was made with lots of funding through until the 70's, where people then realised
Artificial Intelligence was very hard, causing funding to dry up, and the term
`AI Winter' to be coined.

In the 80's people started to look at biologically inspired algorithms such as
neural networks and genetic algorithms, and there is more investment. This leads
to the field of AI diverging into many other fields such as Computer Vision,
NLP, Machine Learning etc. In the 00's, ML begins to overlap with statistics,
and the first {\it useful} applications emerge.

\section{Nearest Neighbour Classifier}

The Nearest Neighbour (NN) classifier is a simple implementation of a machine
learning algorithm. We can give it a set of training data with each point
labelled by a class, and then give it another datapoint, and that datapoint will
be classified according to the data.

The premise is that you plot all the points of the training data on a graph, and
when you want to classify another datapoint, you simply plot it on the existing
graph, and classify it by the classes of it's nearest neighbour(s).

% NN graph

One nice aspect of the NN classifier is that you can use it with as many
features as you like with little extra effort. A standard implementation of the
NN classifier might plot graphs in two dimensions, and thus will only be able to
classify data with two features. However, if you want to include more (or less)
features, you need to adjust the number of dimensions on your graph to match the
number of features you're including. Then when you find the nearest neighbours
on the graph, you find them in $n$-dimensional space rather than 2-dimensional
space, where $n$ is the number of dimensions on the graph.

\subsection{Finding the distance between two $n$-dimensional points}

In order to calculate which points in the training dataset are the nearest
neighbours to the new data point, we can calculate the {\it Euclidean Distance}
between the two points. In $n$ dimensions, this how:

\[
  d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_{n-1} - q_{n-1})^2 + (p_n - q_n)^2}
\]

This is actually a very simple computation. If you're given two arrays
(containing the coordinates of your points) of equal lengths, then you can do
something similar to Listing~\ref{scala_nn}.

\begin{lstlisting}[label=scala_nn, caption=Scala Euclidean Distance]
def euclideanDistance(c1: List[Double], c2: List[Double]): Double = {
  val delta: Set[Double] = for {(a,b) <- c1 zip c2} yield math.pow(a-b, 2)
  Double = math.sqrt(delta.sum)
}
\end{lstlisting}

The reason why using KNN on n-dimensions is that you can classify different
types of data. A 2d KNN classifier could classify on two variables, such as
weight and height, while a 256d classifier could classify a 16x16 monochrome
image where each pixel is represented by a dimension.

\subsection{Computing the nearest neighbour}

Computing the nearest neighbour to a point is simply as easy as finding the point
in the training data that has the smallest euclidean distance to it, as shown in
Listing~\ref{nearestNeighbour}.

\begin{lstlisting}[label=nearestNeighbour, caption=Scala Nearest Neighbour]
def nearestNeighbour(input: Point, existing: Set[Point]): Point = {
  existing.min((p: Point) => p.euclideanDistance(input))
}
\end{lstlisting}

\marginpar{Assume that there is a custom implementation of {\tt Point} that
exposes the method to find the euclidean distance between it and another point,
similar to that in Listing~\ref{scala_nn}}

\subsection{Multiple nearest neighbours (K-NN)}

It's possible that more accurate classifications of points could be obtained by
taking into account multiple close neighbours rather than one nearest one. In
order to do this, you need to find the number of occurrences of a specific class
in the top $K$ nearest neighbours:

%TODO: Check this compiles

\begin{lstlisting}[label=nearestNeighbourk, caption=Scala Nearest Neighbour]
def kNearestNeighbour(k: Int, input: Point, existing: Set[Point]): Class = {
  val topK: List[Class] = existing.toList.sorted(
      (p: Point) => p.euclideanDistance(input)
    ).take(k).map(_.class)
  topK.groupBy(identity).maxBy(_._2.size)._1
}
\end{lstlisting}

One must be careful when choosing a value of $K$ for the classifier. As $K$
increases proportional to the size of the dataset, then the number of incorrect
classifications will increase.

Figure~\ref{fig:knn-large} shows two identical datasets that a KNN
classification is being applied to. When $K = 3$, the correct classification of
a square is made, and when $K = 5$, the wrong classification is made, simply
because there are more circles than squares in the dataset.

\begin{figure}[!h]
  \includestandalone[width=\textwidth]{diagrams/knn-large}
  \caption{How K affects classification accuracy. The item being classified is
  the filled square and nearest neighbours are filled, and other elements are
  left unfilled. When K=3, it would be classified as square, when K=5, it'd be
  classified as a circle.}
  \label{fig:knn-large}
\end{figure}

With a nearest neighbour classifier, there is always one or more
\textit{boundaries} that defines which class a new item will be placed in, as
shown in Figure~\ref{fig:knn-boundary}. Be aware that the \textbf{Decision
Boundary} isn't always contiguous or a straight line.

\begin{figure}[!h]
  \centering
  \includestandalone[width=0.4\textwidth]{diagrams/knn-boundary}
  \caption{The dotted line represents the decision boundary for a KNN
  classifier}
  \label{fig:knn-boundary}
\end{figure}

\subsection{Overfitting}

Overfitting is a problem with all ML algorithms, and occurs when an algorithm is
trained on a small proportion of the dataset, that isn't Representative of the
whole data. It can also be when the algorithm is trained in such a way that it
describes the noise or random error in the training set.

Avoiding overfitting is hard, but generally it involves choosing the right
algorithm, the right parameters and the right training data.

\section{Linear Classifier}

The nearest neighbour algorithm requires a minimal amount of logic in the
training stage, since it's really only the value of $K$ that needs to be
considered. In fact, for a simple knn classifier, you don't even need to train
it, since you could just pick an arbitrary value for $K$, and then all the work
of the algorithm would be done at the `Model' stage? However, most other ML
algorithms require some kind of learning phrase before they can become useful.
The simplest of these is probably the linear classifier.

A simple linear classifier is one that looks at only one parameter, and can
classify into two classes. Essentially it says:

\begin{lstlisting}[label=linearClassifier, caption=A simple linear classifier]
if(parameter > threshold) class1 else class2
\end{lstlisting}

This classifier would work well for some limited use cases; maybe if we were
classifying boxers into different weight categories. However, we need to decide
on a threshold value in order for the classifier to work. In the case of boxing
this is easy, since there are known weight categories, however, in some other
cases, we might have to discover the threshold value from the data.

If there are two classes in the data which are linearly separable (i.e. there is
one or more threshold(s) that will accurately tell them apart), then an
algorithm to tell them apart is easy to write so long as you know the correct
threshold value. In order to find the right threshold value, you need a learning
algorithm such as this:

\begin{lstlisting}[label=linearClassifierLearningAlgorithm, caption=Linear classifier learning algorithm]
var errors: Int = 0
var threshold: Int = 0;
do {
  // classify will use a linear classifier to classify the data with a specific
  // threshold, and return the number of errors.
  errors = classify(data, correctLabels, threshold++)
} while(errors > 0);
\end{lstlisting}

In all learning algorithms, an error function is needed to evaluate whether the
changes to the parameters for the classifier on each iteration have had a
positive or negative effect on the accuracy of classification. The error
function in this case is \texttt{while(errors > 0)}, since we are assuming that
the data is linearly separable, we can just keep incrementing the threshold
value until we get a one that produces no errors.

\begin{figure}[!h]
  \includestandalone[width=\textwidth]{diagrams/generic-ML-algorithm}
  \caption{A generic ML algorithm always has the above structure.}
  \label{fig:generic-ML-algorithm}
\end{figure}

\section{Perceptron}

A perceptron is an ML algorithm that mimics a single neuron in a brain. Despite
emulating only one neuron, a perceptron can accurately classify a wide variety of
data.

The algorithm has $d$ inputs ($i_0, \dots, i_d$), and $d$ weights ($w_0, \dots,
w_d$), where each input has a specific weight associated with it. It also has
another parameter $t$, which is the classification threshold. The algorithm can
be expressed easily in mathematical notation:

\[
  classify(I) = 
  \begin{cases}
    1 & \text{if } t \leq \sum\limits^d_{j=0}i_jw_j\\
    0 & \text{otherwise}
  \end{cases}
\]

For the less mathematically inclined, here's the MATLAB code that does the same
thing:

\begin{lstlisting}[language=matlab, label=perceptron,
                   caption=A perceptron implementation in MATLAB]
  function output = perceptron(threshold, inputs, weights)

  % Compute the activiation level
  activation_level = sum(inputs.*weights);

  if activation_level > threshold
    output = 1;
  else
    output = 0;
\end{lstlisting}

Therefore:

\begin{verbatim}
  $ inputs = [0.1, 0.4, 0.2]
  $ weights = [0.9, 0.1, 0.7]
  $ threshold = 0.14
  $ perceptron(threshold, inputs, weights)
    > 1
  $ threshold = 0.32
  $ inputs = [0.1, 0.6, 0.1]
  $ perceptron(threshold, inputs, weights)
    > 0
\end{verbatim}

Training a perceptron is a matter of adjusting the weights and the threshold to
get the best classification accuracy on the training data. We need a rule that
we can apply over and over again to the parameters to refine them towards better
values:

\[
  weight = weight \times \text{\it learning\_rate} \times (actual - output) \times input
\]

Henceforth, we can create a learning algorithm for the perceptron. Lines 5 and 6
of Listing~\ref{perceptronTraining} are the implementation of the perceptron
learning rule, everything else is the logic to loop over all the weights a
certain number of times.

\begin{lstlisting}[language=java, label=perceptronTraining,
                   caption=A perceptron learning algorithm in Java]
  for(int i = 0; i < iterations; i++) {
    for(int example = 0; example < trainingSet.length; example++) {
      int output = perceptron(threshold, trainingSet[example], weights);
      for(int weight = 0; weight < weights.length; weight++) {
        int scaleFactor = learningRate * (trainingLabels[example] - output);
        weights[weight] = weights[weight] * trainingSet[example][weight] * learningRate;
      }
    }
  }
\end{lstlisting}

Note how computationally intensive the training algorithm is. The time
complexity is $O(i \cdot t \cdot w)$ where $i$ is the number of training
iterations to do, $t$ is the number of training examples, and $w$ is the number
of weights/inputs/dimensions to classify with.

Even though training is expensive, classification is relatively cheap, with a
linear runtime.

There is a theorem, called the \textbf{Perceptron Convergence Theorem}, that
states ``if the data is linearly separable, then application of the perceptron
learning rule will find a decision boundary within a finite number of
iterations''.

\subsection{Multilayer Perceptrons (MLP)}

A a multilayer perceptron is basically a graph structure, where every node is a
perceptron, and edges are connections from the output of one perceptron to the
input of another. In order to make this work, a different type of perceptron is
often used. Instead of having a threshold value that is compared to the sum of
the weighted inputs, the sum is run through a sigmoid function before being
output.

With a network of perceptrons, the decision boundary can now be curved and
doesn't have to be linear (as it is with only one perceptron). This means more
complex problems can be attempted.

In order to train a neural network, a technique called \textit{backpropogation}
is used. However, this is beyond the scope of this course.


\section{Decision trees}

% (Linear classifier == Decision stump) because it's like a decision tree with one level

A decision tree is a tree of questions, where the answers to each question will
either lead to another question, or a classification/answer. They are good at
handling categorical data, and worse at handling continuous data, since they
require a specific answer to progress to the next level of the tree.

\subsection{Building a decision tree}

An algorithm to build a decision tree is relatively easy to come up with.
Listing~\ref{createDecisionTree} shows an example algorithm (adapted from the
course notes).

\begin{lstlisting}[language=java, label=createDecisionTree,
                   caption=An algorithm (the ID3 algorithm) to produce a
                  decision tree in Java]
  Tree learnTree(data) {
    if(isAllSameLabel(data) != null) {
      return new Leaf(isAllSameLabel(data));
    } else {
      Tree out = new Tree();
      Feature importantFeature = extractImportantFeature(data);
      for(Value v : importantFeature.values) {
        out.addBranch(v, learnTree(importantFeature.rowsWithValue(data, v)));
      }
      return out;
    }
  }
\end{lstlisting}

This is kind of understandable, but there are some quirks. How do we extract an
important feature?

\subsubsection{Entropy}

Entropy is the amount of information contained in a variable, given the symbol
$H$.

\[
  H(x) = -\sum\limits_ip(x_i)log_2p(x_i)
\] 

We measure entropy in bits, since we're using log of base 2.

When we're choosing an important feature, we want to reduce the entropy in the
system, so that we gain the maximum amount of information. The best feature to
choose is the one where the $H(T) - H(T | F)$ is largest, as defined by:

\marginpar{$F$ is a feature, such as windy, or bottle size...}

\begin{align*}
  H(T) &= \text{The entropy before the split}\\
  H(T | F_1) &= \text{The entropy of the data on the first branch}\\
  H(T | F_n) &= \text{The entropy of the data on the n'th branch}\\
  H(T | F) &= \text{The weighted average of the entropy on all the branches}\\
\end{align*}

If we had the following table:

\begin{table}[H]
  \centering
  \begin{tabular}{l l l}
    \textbf{Colour} & \textbf{Bottle Size} & \textbf{Class}\\
    Red     & Big   & Wine\\
    Red     & Big   & Beer\\
    Yellow  & Small & Cider\\
    White   & Big   & Wine\\
    Yellow  & Small & Beer\\
  \end{tabular}
\end{table}

We can compute $H(T)$:

\begin{align*}
  H(T) &= -\sum\limits_ip(x_i)log_2p(x_i)\\
       &= -(\frac{1}{5}log_2\frac{1}{5} + \frac{2}{5}log_2\frac{2}{5} + \frac{2}{5}log_2\frac{2}{5})\\
       &= 1.52193
\end{align*}

If we choose the size to be our next question:

\begin{align*}
  H(T | S = Small) &= -\left(\frac{1}{2}log_2\frac{1}{2} + \frac{1}{2}log_2\frac{1}{2}\right)\\
                   &= 1\\
  H(T | S = Big)   &= -\left(\frac{1}{3}log_2\frac{1}{3} + \frac{2}{3}log_2\frac{2}{3}\right)\\
                   &= 0.91830\\
  H(T | S)         &= \frac{2}{5}H(T | S = Small) + \frac{3}{5}H(T | S = Big)\\
                   &= 0.95098
\end{align*}

If we chose colour:

\begin{align*}
  H(T | C = Red)   &= -\left(\frac{1}{2}log_2\frac{1}{2} + \frac{1}{2}log_2\frac{1}{2}\right)\\
                   &= 1\\
  H(T | C = Yellow)&= -\left(\frac{1}{2}log_2\frac{1}{2} + \frac{1}{2}log_2\frac{1}{2}\right)\\
                   &= 1\\
  H(T | C = White) &= -\left(\frac{1}{1}log_2\frac{1}{1}\right)\\
                   &= 0\\
  H(T | C)         &= \frac{2}{5}H(T | C = Red) + \frac{2}{5}H(T | C = Yellow) + \frac{1}{5}H(T | C = White)\\
                   &= 0.8
\end{align*}

What should we choose then? Well:

\begin{align*}
  H(T) - H(T | C) &= 1.52193 - 0.8 &= 0.72193\\
  H(T) - H(T | S) &= 1.52193 - 0.95098 &= 0.57095\\
\end{align*}

Therefore we should choose colour to be our first question, since it has the
maximum value of $H(T) - H(T|F)$

\subsection{Overfitting decision trees}

It is important to ensure that decision trees are not overfitted. The most
extreme case of overfitting, is when you have $n$ rules, and $n$ pieces of data
in your dataset (i.e. you have a rule for every piece of data).

In order to stop overfitting, we can either stop generating the tree after a
certain depth (which also keeps it small and efficient), or we can prune the
tree after we've built it to make it smaller.

Figure~\ref{fig:overfitting} shows when it's best to start pruning the decision
tree.

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[ 
      xlabel=Depth of Tree,
      ylabel=Error Rate,
      yticklabels={,,},
      xticklabels={,,},
      after end axis/.append code={
        \draw [thick] (200,0) -- (200,500);
      }
    ] 
      % Revision procrastination = coming up with functions to describe the
      % curves properly!
      \addplot[mark=none, color=red][domain=2:6] {0.2*(x-3)^2 + 5}; 
      \addplot[mark=none, color=blue, dashed][domain=2:6] {-0.1 * x^2 + 5}; 
    \end{axis}
  \end{tikzpicture}
  \caption{The vertical line shows the cutoff point, where the error rate for
  testing with the validation data (red, unbroken) begins to rise, while the
  error rate for the training data (blue, dashed), which we're generating the
  decision tree from, is still improving. It is here that we should prune the
  tree.}
  \label{fig:overfitting}
\end{figure}

\section{Learning experiments in Machine Learning}

The way we train our ML algorithms, is to get all the data that we have access
to and split it in half. Then, we can train on the first half and test our
parameters on the second half. Often we don't split the data into equal halves,
but make the training set smaller than the testing set.

When we test our algorithm on the datasets, we usually get a height `testing
error' than `training error'. This is because we have trained our ML algorithm
on the training data, so it will usually be better at classifying it.

\subsection{Cross validation}

To make better use of our data, we could split our \textbf{training data} into
$n$ chunks, train on $n-1$ chunks and test on the one that has been left out,
before rotating the chunks so that a different one is for testing. Then, see
which parameters were best in our training rounds, and use them on the training
data.

The aim is to make sure that all the data has an equal chance of appearing in
the training or the testing set.

\subsection{Dealing with misclassifications}

If we had one class that was very rare, and another that was almost ubiquitous,
then it would be hard to train our algorithm, since we would always get low
error rates by always classifying as the most common class.

The solution to this is to measure the accuracy on each class separately, and
try to get a distributed, but low error rate over all the classes.

We can create a confusion matrix to analyse our errors:

\begin{table}[H]
  \centering
  \begin{tabular}{lc|c|c}
    & \multicolumn{3}{c}{Prediction} \\
    & & Class 1 & Class 2 \\ \cline{2-4}
    \multirow{2}{*}{Actual value} & Class 1 & Correct1 & False-Positive1\\ \cline{2-4}
    & Class 2 & False-Positive2 & Correct2\\
  \end{tabular}
\end{table}

If Class 2 was the rare one, then we could calculate our \textit{sensitivity}
(the chances of correctly classifying it when given it) by doing:

\[
  \frac{Correct2}{Correct2 + False-Positive2}
\]

We could calculate our \textit{specificity}, which is the chance of correctly
classifying a member of Class 2 (the common one) by doing:

\[
  \frac{Correct1}{Correct1 + False-Positive1}
\]

Confusion Matrices are also useful if falsely classifying one class is worse
than falsely classifying another, since you can work out the cost of false
classifications:

\[
  cost = (\#False-Positive-1 \times cost1) + (\#False-Positive-2 \times cost2)
\]

This is called ROC (Receiver Operator Characteristics). It was developed in
world war two (hence the weird name) to help analyse radar classifications of
bombers.

\section{Ensemble Learning Algorithms}

An Ensemble System fits multiple models (called base learners) to the training
data, and when classifying, is uses the models as a \textit{committee} to vote
on the testing data.

In order to create a useful ensemble, the members (different models) can't be
identical; they need to disagree. The amount of disagreement is called the
diversity. We don't want too much disagreement, since then we won't be able to
make a decision.

The way to create diverse members is to use a slightly different training
dataset for each model. Cross validation and feature extraction can be good for
this.

Not all models are suitable as base learners, if the model only changes by a
small amount when the training data is changed, then it won't have much
diversity. To ensure the ensemble is diverse, the base learner should produce
large changes to it's model for smaller changes in the training data. A good
base learner is therefore \textit{unstable}, an example of such an algorithm is
the decision tree.

\subsection{Bootstrapping}

Bootstrapping is a way of generating multiple datasets from an original
dataset. You select $N$ training examples from the total $N$ with replacement.

This means that on average, $36.8\%$ of points are unselected.

\subsection{Bagging}

BAGGING is an acronym for Bootstrap AGGregatING. It's an ensemble method that
generates $m$ bootstraps, and trains $m$ models on each. In order to classify
new data, a simple majority vote is used.

Bagging is a parallel algorithm; ensemble members work independently to reach a
conclusion and then the results are collated.

\subsection{Boosting}

Boosting is basically bagging, except the bootstrapping part is on roids:

\begin{enumerate}
  \item Take a bootstrap of the dataset.
  \item Train one model on the bootstrap.
  \item Take a look at which examples the model gets wrong.
  \item Upweight the `hard' examples and downweight the `easy' ones.
  \item Go back to step 1, but with a weighted bootstrap until you run out of
    models to train.
\end{enumerate}

This means that each new member of the ensemble focuses on thee ones that the
previous member got wrong. It's a serial algorithm, since members are trained
one after the other rather than at the same time.

\section{Types of ML classifiers}

Ke Chen is very keen to emphasise the following:

\begin{description}
  \item \textbf{Discriminative} classifiers both model a classification rule
    directly (such as a decision tree), and model the probability of class
    memberships based on input data.
    \marginpar{Generative classifiers are always probabilistic classifiers (at
    least it seems that way in his notes!)}
  \item \textbf{Generative} classifiers make a probabilistic model of data
    within each class.
  \item \textbf{Probabilistic} classifiers use probabilities to classify data.
\end{description}

If that's not very clear, discriminative classifiers give a probability for
\textit{each} class based on input data, while generative classifiers are
trained on a specific class, and give a probability for that class.

\section{Naive Bayes Classifier}

Rather than me warbling on about this; have a gander at what Sebastian Raschka
has to say instead. He has prettier diagrams, and a better understanding than I
do:

\begin{itemize}
  \item \url{http://sebastianraschka.com/Articles/2014_naive_bayes_1.html}

  \item \url{http://sebastianraschka.com/PDFs/articles/naive_bayes_1.pdf}
\end{itemize}

\section{Clustering Analysis}

A \textbf{cluster} is a collection of data objects or points that are similar to
one another. Objects in other clusters will be dissimilar to objects in this
cluster.

\marginpar{\textbf{Unsupervised} learning algorithms try and find hidden
structure in unlabelled data. Unlike supervised learning algorithms, there is no
way to evaluate potential solutions, since the data is unlabelled.}

\textbf{Cluster analysis} involves finding similarities between data and
grouping similar data points into clusters. It's a type of \textit{unsupervised
learning}, and is used both as a stand alone tool to gain an insight into data,
but also as a pre-processing step for other algorithms.

As a simple example, you could split a list of animals (\textit{dog, cat, shark,
minnow, frog, worm}) into ones that have legs (\textit{dog, cat, frog}) and ones
that don't (\textit{shark, minnow, worm}). This is a trivial example, but the
technique is used in many fields:

\begin{mymulticols}
  \begin{itemize}
    \item \textbf{Banking/Internet Security} (Fraud detection)
    \item \textbf{Biology} (Classification of species)
    \item \textbf{Climate Change} (Better understanding the climate, finding
      atmospheric patterns)
    \item \textbf{Finance} Uncover correlation in underlying shares
    \item \textbf{Image Compression} (group similar pixels)
    \item Many more\dots
  \end{itemize}
\end{mymulticols}

\subsection{Representing data}

\subsubsection{Data matrix}

The easiest way to represent data is using a matrix, where the rows represent
each data point, and the columns represent each feature. It's good at
representing data with $n$ datapoints (rows) and $p$ dimensions (columns). Also
known as \textit{object by feature}. Data matrices can be said to have two
`modes' since their rows and columns can be interchanged.

\subsubsection{Distance matrix}

This is a triangular matrix that describes the distance between points.

\begin{center}
  \begin{tabular}{lllll}
      & A & B & C & D\\
    A & 0 & 4 & 6 & 3\\
    B & 4 & 0 & 2 & 7\\
    C & 6 & 2 & 0 & 5\\
    D & 3 & 7 & 5 & 0
  \end{tabular}
\end{center}

You can see that the matrix is symmetrical, so we generally only show the bottom
left side (to save confusion and space). If you shade the cells of the matrix in
according to their value, you can get a heat map of the data, which can reveal
hidden patterns.

You may not have considered that there may be different ways of computing the
distance between coordinates, but we will cover three ways in this course. In
the following list, $n$ is the number of data points, and $p$ is the number of
features:

\begin{description}
  \item \textbf{Euclidean Distance}\\ 
    $d(a,b) = \sqrt{|a_1 - b_1|^2 + \dots + |a_n - b_n|^2}$
  \item \textbf{Manhattan Distance}\\
    $d(a,b) = |a_1 - b_1| + \dots + |a_n - b_n|$\\
    This one would find the distance you'd have to go if you could only go in
    straight lines, and turn in $90^{\circ}$ increments (so it's the distance
    you'd have to walk in Manhattan).
  \item \textbf{Minkowski Distance}\\
    $d(a,b) = \sqrt[\leftroot{-2}\uproot{2}p]{|a_1 - b_1|^p + \dots + |a_n - b_n|^p}, p > 0$
\end{description}

\subsubsection{Cosine Similarity}

Cosine similarity is a measure of similarity between two vectors in $n$
dimensional space. It measure the cosine of the angle between the vectors;
$cos(0^{\circ}) = 1$, while any angle other than $0^{\circ}$ is less than $1$.
It is therefore a measure of the \textit{orientation, irrespective of the
magnitude} of the two vectors. Diametrically opposed vectors will have a
similarity of -1.

In order to calculate the cosine similarity, you use this formula:

\[
  \cos(x,y) = \frac{x \cdot y}{\|x\| \|y\|} = \frac{ \sum\limits_{i=1}^{n}{x_iy_i} }{ \sqrt{\sum\limits_{i=1}^{n}{(x_i)^2}} \times \sqrt{\sum\limits_{i=1}^{n}{(y_i)^2}} }
\]

In order to find the \textbf{distance} between two vectors using cosine
similarity, you do:

\[
  d(x,y) = 1 - cos(x,y)
\]  

One example use of the cosine similarity, is in text mining, where if you have
documents represented as a vectors, where each word is given a dimension that
represents the number of times it appears in the document, cosine similarity can
find how similar to each other, documents are.

Here's an example:

\begin{align*}
  x &= (1,2,3,4,5)\\
  y &= (0,3,4,7,9)\\
  x \cdot y &= (1 * 0) + (2 * 3) + (3 * 4) + (4 * 7) + (5 * 9)\\
           &= 0 + 6 + 12 + 28 + 45\\
           &= 91\\
  \|x\| &= \sqrt{1^2 + 2^2 + 3^2 + 4^2 + 5^2}\\
        &= \sqrt{1 + 4 + 9 + 16 + 25}\\
        &\approx 7.416\\
  \|y\| &= \sqrt{0^2 + 3^2 + 4^2 + 7^2 + 9^2}\\
        &= \sqrt{9 + 16 + 49 + 81}\\
        &\approx 12.450\\
  cos(x,y) &= \frac{91}{7.416 \cdot 12.450} = 0.986\\
  d(x,y) &= 1 - 0.986 = 0.14
\end{align*}

\subsubsection{Measuring the distance of binary features}

You can measure the distance between binary features by converting them all to
$1$ and $0$ (if you've not already), and draw a \textit{contingency table} for
them:

\begin{table}[H]
  \centering
  \begin{tabular}{lc|cc}
    & \multicolumn{1}{l}{} & \multicolumn{2}{c}{y} \\
    & & 1 & 0 \\ \cline{2-4}
    \multirow{2}{*}{x} & 1 & a & b\\
    & 0 & c & d\\
  \end{tabular}
\end{table}

Where:
\begin{itemize}
  \item $a$ is the number of features that are 1 for $x$ and $y$
  \item $b$ is the number of features that are 0 for $x$ and 1 for $y$
  \item $c$ is the number of features that are 1 for $x$ and 0 for $y$
  \item $d$ is the number of features that are 0 for $x$ and $y$
\end{itemize}

To find the actual distance for a symmetric binary attribute (where both classes
are equal (e.g. male and female)):

\[
  d(x,y) = \frac{b + c}{a + b + c + d}
\]

If the attributes are asymmetric (one is more rare than the other, say if
the test for a disease is positive (important) or negative (less important)),
set the rarest one to $1$ and do:

\[
  d(x,y) = \frac{b + c}{a + b + c}
\]

For example:

\begin{center}
  \begin{tabular}{l llllll}
    Name & Dairy & Pollen & Gluten & Fish & Mushrooms & Peanuts\\ \hline
    Bob  & 1     & 0      & 0      & 0    & 1         & 0      \\
    Bill & 0     & 1      & 0      & 0    & 1         & 1      
  \end{tabular}
\end{center}

To find the distance (these would be asymmetric binary attributes), you do:

\[
  d(bob, bill) = \frac{1 + 2}{1 + 2 + 1} = \frac{3}{4} = 0.75
\]  

\subsection{Distance for nominal features}

If the values for dimensions are discrete, we can convert them into more binary
dimensions. For example, if we had one colour dimension, that had $\{red, blue,
green, black\}$ as it's values, we could convert that into four binary
dimensions (and then condense is back into one dimension (e.g. 1000, or 0100),
and do binary distance measuring on it.

A simpler (but worse) way of finding the distance between two nominal entities
is to do:

\[
  d(x,y) = \frac{\text{number of mis-matching features}}{\text{total number of features}}
\]

\subsection{Clustering approaches}

These are the main clustering approaches:

\begin{description}
  \item \textbf{Partitioning approach}\\
  You can partition the data with a view to evaluating all of the partitions by
  some criteria (e.g. minimising the sum of distances). K-means and k-medoid
  does this.

  \item \textbf{Hierarchical approach}\\
  Aim to classify the data into hierarchies (e.g. draw increasingly small circles
  around datapoints). Agglomerative, Diana, Agnes, BIRCH are all examples of
  this.

  \item \textbf{Density approach}\\
  Create a function that will calculate the density or connectiveness of the
  data at a point, and use that to form clusters.

  \item \textbf{Spectral approach}\\
  Convert the data into a weighted graph, and cut the graph into sub-graphs that
  correspond to clusters.

  \item \textbf{Ensemble approach}\\
  Combine multiple clustering techniques to generate clusters.
\end{description}

\subsubsection{K-means clustering}

As previously mentioned, k-means is a partitioning clustering approach. It
iteratively partitions the training data to produce several non-empty clusters.
The idea is to produce optimal partitions in which the sum of the squared
distance to the `representative object' is minimised.

Since partitioning the dataset into $K$ optimal partitions is NP-Hard, the
problem must be solved using heuristic algorithm. The end goal is to minimise
the value of:

\[
  \sum\limits_{k=1}^K\sum\limits_{x \in S_k}d^2(x, m_k)
\]

Where $K$ is the number of partitions to find, $S_k$ is the $k$th partition (a
set) of the data, and $m_k$ is the `Representative object' (or the mean of the
points) in the set $S_i$.

The algorithm works like so:

\begin{enumerate}
  \item Pick $K$ random seed points (from the data).
  \item Assign each data object to the cluster with the nearest seed point
    (measured with whatever distance metric you're using e.g. Manhattan).
  \item Compute the mean points of the clusters (aka centroids).
  \item Go back to Step 1), but the seed points are now the means calculated in
    Step 3). Do this until the assigned sets don't change between iterations.
\end{enumerate}

%TODO: The task from KE's lecture 2, slide 12

% Start from slide 13 of Ke chen's second lecture

When K-means has been applied to a dataset, the K partitions are mutually
exclusive, and therefore partition the dataset. A \textit{Voronoi Diagram} is
created.

K-means is fairly easy to compute using the heuristic algorithm. The runtime is
$O(tKn)$, where $t$ is the number of iterations, $K$ is the number of clusters,
and $n$ is the number of objects. $K$ and $t$ are usually much smaller than $n$.

There are a number of problems with K-means:

\marginpar{K-Medoids can mitigate bullet 3, and K-modes can mitigate bullet 6.}

\begin{itemize}
  \item The initial seed points can cause `local optimum' clusters, which may
    not be representative of the whole data.
  \item The number of clusters $K$ needs to be specified in advance, but may be
    unknown.
  \item Noisy data and outliers mess up the algorithm.
  \item If a cluster has a non-convex (i.e. concave) shape, then it won't be
    detected.
  \item Unable to classify categorical data (unless you modify the algorithm).
  \item It's hard to evaluate the performance of the algorithm.
\end{itemize}

\subsubsection{Hierarchical Clustering}

Hierarchical clustering partitions the data sequentially, construction
partitions layer by layer by grouping objects together into a tree. A distance
matrix is used to determine clusters.

There are two sequential strategies used to construct a tree of clusters:

\marginpar{The Agglomerative approach is a bottom up strategy, whereas the
divisive approach is a top down strategy.}

\begin{description}
  \item  \textbf{Agglomerative}\\
    Each data object is in it's own cluster initially, before they are merged
    into progressively larger clusters.
  \item \textbf{Divisive}\\
    All objects start in a single cluster, which is then divided into smaller
    and smaller clusters.
\end{description}

You can measure the distance between clusters in three ways:

\begin{description}
  \item \textbf{Single Link}\\
    Use the \textit{smallest} distance between an element in one cluster and an
    element in another.
  \item \textbf{Complete Link}\\
    Use the \textit{largest} distance between an element in one cluster and an
    element in another.
  \item \textbf{Average}\\
    Find the average distance between the points in the two clusters and use
    that.
\end{description}

For example, given a set of named one dimensional locations
$\{a=1,b=3,c=7,d=8,e=9\}$, and the clusters $C_1 = \{a,b\}, C_2 = \{c,d,e\}$,
find the distances between them:

\begin{align*}
  \text{Single Link}   &= min(d(a,c),d(a,d),d(a,e),d(b,c),d(b,d),d(b,e))\\
                       &= min(6,7,8,4,5,6) = 4\\
  \text{Complete Link} &= max(d(a,c),d(a,d),d(a,e),d(b,c),d(b,d),d(b,e))\\
                       &= max(6,7,8,4,5,6) = 8\\
  \text{Average Link}  &= \frac{(d(a,c)+d(a,d)+d(a,e)+d(b,c)+d(b,d)+d(b,e))}{6}\\
                       &= \frac{6+7+8+4+5+6}{6} = 6
\end{align*}

One issue with hierarchical approaches is what value to choose for $K$ (when
we've found enough clusters). One heuristic is to stop processing when the
distance between clusters reaches a certain threshold.

\subsubsection{Hierarchical - Agglomerative}

The agglomerative algorithm is as follows:
  
\begin{itemize}
  \item Convert the object attributes into a distance matrix, and make each
    object a cluster (of size 1).
  \item Merge the two closest clusters together
  \marginpar{Since we have fewer clusters as the algorithm runs for longer,
  updating the distance matrix will take less and less time (if you can't
  work out why, check out slide 10 of Ke's second lecture.}
  \item Update the distance matrix to take into account the new cluster
  \item Repeat from step two until we've got the desired number of clusters.
\end{itemize}

You can show the creation of clusters with a dendrogram, as in
Figure~\ref{fig:dendrogram}. The $x$ axis in a dendrogram is the datapoints, and
the $y$ axis is the distance. The lines merge when the agglomerative algorithm
merges two clusters, and the number under the horizontal merge line indicates the
$i$th merge. The data that would produce such an algorithm is in
Table~\ref{data1}.

%TODO: The exercise on slide 18 of Ke's second lecture.

\begin{table}[h]
  \centering
  \begin{tabular}{llllll}
      & a  & b & c & d & e\\ \hline
    a & 0  & 3 & 6 & 7 & 9\\
    b & 3  & 0 & 3 & 4 & 6\\
    c & 6  & 3 & 0 & 1 & 3\\
    d & 7  & 4 & 1 & 0 & 2\\
    e & 9  & 6 & 3 & 2 & 0
  \end{tabular}
  \caption{A distance matrix for a hierarchical clustering algorithm.}
  \label{data1}
\end{table}

\begin{figure}[h]
  \includestandalone[width=\textwidth]{diagrams/dendrogram}
  \caption{A dendrogram produced from running the agglomerative algorithm on the
  data in Table~\ref{data1}.}
  \label{fig:dendrogram}
\end{figure}

The agglomerative approach has some weaknesses; it's very sensitive to noise and
outlying datapoints, and it is less (usually) efficient than k-means clustering,
with a runtime of $O(n^2)$.

\subsection{Cluster validation}

Cluster validation is the art of evaluating the results of clustering algorithms
in a quantitative and objective manner.

\subsubsection{Internal Indexes}

Internal indexes are used to validate the value of $K$ for our clustering
algorithms when the `ground truth' is unavailable. \marginpar{\textbf{Ground
Truth} is the term used to describe the training data that we can use for our
algorithm} Because of this, common sense and prior knowledge is used to validate
the clusters.

\marginpar{Intra-cluster = within a cluster. Inter-cluster = over multiple
clusters. In this context, a low intra-cluster, and high inter-cluster variance
will make for more disjoint, widely spaced, yet tightly packed clusters.}

We can use variance to evaluate the clusters, checking to see that the intra-
cluster variance is minimised, and the inter-cluster variance is maximised.

This analysis generates an `F-ratio index' that we can use to determine how well
our clustering algorithms have done. In order to calculate the F-ratio index, we
the divide the intra-cluster variance by the inter-cluster variance:

\[
  F(m) = \frac{mSSW(m)}{SSB(m)} = \frac{m\sum\limits_{i=1}^m\sum\limits_{j=1}^{n_i}d^2(x_{ij},c_i)}{\sum\limits_{i=1}^mn_id^2(c_i,c)}
\]

While this algorithm looks like something that walked out of Euler's notebook,
it's actually not that hard:

\begin{center}
  \begin{tabular}{ll}
    Variable & Meaning\\
    $m$     & The number of clusters generated by the algorithm.\\
    $n_i$   & The number of datapoints in the $i$th cluster.\\
    $c_i$   & The centroid for the $i$th cluster.\\
    $x_{ij}$& The $j$th datapoint in cluster $c_i$.\\
    $c$     & The mean centroid for the whole dataset.\\
    $d(x,y)$& The distance function we're using.
  \end{tabular}
\end{center}

The way to use the variance analysis, is to find the number of clusters with the
smallest f-ratio index. To do this, run the clustering algorithm ranging from
$K$ (the number of clusters) set as 2, up to $n$ (an arbitrary limit). Then find
the F-ratio index for each $K$ value, and use the value with the lowest F-ratio
index.

\subsubsection{External Indexes}

When the ground truth is available, but our clustering algorithm doesn't make
use of the information (probably because it's unsupervised), we can evaluate the
effectiveness of the clustering algorithm using external indexes. Unlike internal
indexes, which find a good value for $K$, external indexes are used to find a
performance evaluation for our clustering efforts.

%TODO: More issues of external issues

One issue of external indexes, is that if we have a different number of classes
to the number of clusters, then we'll probably get a lower score for our
clusters.

%TODO: Work out what's going on here...

We can use the Rand Index do this, however, I don't understand enough of the
course notes to write a well explained description here.
