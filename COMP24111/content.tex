% Set the author and title of the compiled pdf
\hypersetup{
	pdftitle = {\Title},
	pdfauthor = {\Author}
}

\section{Machine Learning}

Machine Learning is the creation of self-configuring data structures that allow
a computer to do things that would be classed as `intelligent' if a human did
it.

Machine learning has been around in it's infancy since the 40's, where reasoning
and logic were first studied by Claude Shannon and Kurt Godel. Steady progress
was made with lots of funding through until the 70's, where people then realised
Artificial Intelligence was very hard, causing funding to dry up, and the term
`AI Winter' to be coined.

In the 80's people started to look at biologically inspired algorithms such as
neural networks and genetic algorithms, and there is more investment. This leads
to the field of AI diverging into many other fields such as Computer Vision,
NLP, Machine Learning etc. In the 00's, ML begins to overlap with statistics,
and the first {\it useful} applications emerge.

\section{Nearest Neighbour Classifier}

The Nearest Neighbour (NN) classifier is a simple implementation of a machine
learning algorithm. We can give it a set of training data with each point
labelled by a class, and then give it another datapoint, and that datapoint will
be classified according to the data.

The premise is that you plot all the points of the training data on a graph, and
when you want to classify another datapoint, you simply plot it on the existing
graph, and classify it by the classes of it's nearest neighbour(s).

% NN graph

One nice aspect of the NN classifier is that you can use it with as many
features as you like with little extra effort. A standard implementation of the
NN classifier might plot graphs in two dimensions, and thus will only be able to
classify data with two features. However, if you want to include more (or less)
features, you need to adjust the number of dimensions on your graph to match the
number of features you're including. Then when you find the nearest neighbours
on the graph, you find them in $n$-dimensional space rather than 2-dimensional
space, where $n$ is the number of dimensions on the graph.

\subsection{Finding the distance between two $n$-dimensional points}

In order to calculate which points in the training dataset are the nearest
neighbours to the new data point, we can calculate the {\it Euclidean Distance}
between the two points. In $n$ dimensions, this how:

\[
  d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_{n-1} - q_{n-1})^2 + (p_n - q_n)^2}
\]

This is actually a very simple computation. If you're given two arrays
(containing the coordinates of your points) of equal lengths, then you can do
something similar to Listing~\ref{scala_nn}.

\begin{lstlisting}[label=scala_nn, caption=Scala Euclidean Distance]
def euclideanDistance(c1: List[Double], c2: List[Double]): Double = {
  val delta: Set[Double] = for {(a,b) <- c1 zip c2} yield math.pow(a-b, 2)
  Double = math.sqrt(delta.sum)
}
\end{lstlisting}

The reason why using KNN on n-dimensions is that you can classify different
types of data. A 2d KNN classifier could classify on two variables, such as
weight and height, while a 256d classifier could classify a 16x16 monochrome
image where each pixel is represented by a dimension.

\subsection{Computing the nearest neighbour}

Computing the nearest neighbour to a point is simply as easy as finding the point
in the training data that has the smallest euclidean distance to it, as shown in
Listing~\ref{nearestNeighbour}.

\begin{lstlisting}[label=nearestNeighbour, caption=Scala Nearest Neighbour]
def nearestNeighbour(input: Point, existing: Set[Point]): Point = {
  existing.min((p: Point) => p.euclideanDistance(input))
}
\end{lstlisting}

\marginpar{Assume that there is a custom implementation of {\tt Point} that
exposes the method to find the euclidean distance between it and another point,
similar to that in Listing~\ref{scala_nn}}

\subsection{Multiple nearest neighbours (K-NN)}

It's possible that more accurate classifications of points could be obtained by
taking into account multiple close neighbours rather than one nearest one. In
order to do this, you need to find the number of occurrences of a specific class
in the top $K$ nearest neighbours:

%TODO: Check this compiles

\begin{lstlisting}[label=nearestNeighbour, caption=Scala Nearest Neighbour]
def kNearestNeighbour(k: Int, input: Point, existing: Set[Point]): Class = {
  val topK: List[Class] = existing.toList.sorted(
      (p: Point) => p.euclideanDistance(input)
    ).take(k).map(_.class)
  topK.groupBy(identity).maxBy(_._2.size)._1
}
\end{lstlisting}

One must be careful when choosing a value of $K$ for the classifier. As $K$
increases proportional to the size of the dataset, then the number of incorrect
classifications will increase.

Figure~\ref{fig:knn-large} shows two identical datasets that a KNN
classification is being applied to. When $K = 3$, the correct classification of
a square is made, and when $K = 5$, the wrong classification is made, simply
because there are more circles than squares in the dataset.

\begin{figure}[!h]
  \includestandalone[width=\textwidth]{diagrams/knn-large}
  \caption{How K affects classification accuracy. The item being classified is
  the filled square and nearest neighbours are filled, and other elements are
  left unfilled. When K=3, it would be classified as square, when K=5, it'd be
  classified as a circle.}
  \label{fig:knn-large}
\end{figure}

With a nearest neighbour classifier, there is always one or more
\textit{boundaries} that defines which class a new item will be placed in, as
shown in figure~\ref{fig:knn-boundary}. Be aware that the \textbf{Decision
Boundary} isn't always contiguous or a straight line.

\begin{figure}[!h]
  \centering
  \includestandalone[width=0.4\textwidth]{diagrams/knn-boundary}
  \caption{The dotted line represents the decision boundary for a KNN
  classifier}
  \label{fig:knn-boundary}
\end{figure}

\subsubsection{Overfitting}

Overfitting is a problem with all ML algorithms, and occurs when an algorithm is
trained on a small proportion of the dataset, that isn't Representative of the
whole data. It can also be when the algorithm is trained in such a way that it
describes the noise or random error in the training set.

Avoiding overfitting is hard, but generally it involves choosing the right
algorithm, the right parameters and the right training data.
