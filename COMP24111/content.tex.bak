% Set the author and title of the compiled pdf
\hypersetup{
	pdftitle = {\Title},
	pdfauthor = {\Author}
}

\section{Machine Learning}

Machine Learning is the creation of self-configuring data structures that allow
a computer to do things that would be classed as `intelligent' if a human did
it.

Machine learning has been around in it's infancy since the 40's, where reasoning
and logic were first studied by Claude Shannon and Kurt Godel. Steady progress
was made with lots of funding through until the 70's, where people then realised
Artificial Intelligence was very hard, causing funding to dry up, and the term
`AI Winter' to be coined.

In the 80's people started to look at biologically inspired algorithms such as
neural networks and genetic algorithms, and there is more investment. This leads
to the field of AI diverging into many other fields such as Computer Vision,
NLP, Machine Learning etc. In the 00's, ML begins to overlap with statistics,
and the first {\it useful} applications emerge.

\section{Nearest Neighbour Classifier}

The Nearest Neighbour (NN) classifier is a simple implementation of a machine
learning algorithm. We can give it a set of training data with each point
labelled by a class, and then give it another datapoint, and that datapoint will
be classified according to the data.

The premise is that you plot all the points of the training data on a graph, and
when you want to classify another datapoint, you simply plot it on the existing
graph, and classify it by the classes of it's nearest neighbour(s).

% NN graph

One nice aspect of the NN classifier is that you can use it with as many
features as you like with little extra effort. A standard implementation of the
NN classifier might plot graphs in two dimensions, and thus will only be able to
classify data with two features. However, if you want to include more (or less)
features, you need to adjust the number of dimensions on your graph to match the
number of features you're including. Then when you find the nearest neighbours
on the graph, you find them in $n$-dimensional space rather than 2-dimensional
space, where $n$ is the number of dimensions on the graph.

\subsection{Finding the distance between two $n$-dimensional points}

In order to calculate which points in the training dataset are the nearest
neighbours to the new data point, we can calculate the {\it Euclidean Distance}
between the two points. In $n$ dimensions, this how:

\[
  d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_{n-1} - q_{n-1})^2 + (p_n - q_n)^2}
\]

This is actually a very simple computation. If you're given two arrays
(containing the coordinates of your points) of equal lengths, then you can do
something similar to Listing~\ref{scala_nn}.

\begin{lstlisting}[label=scala_nn, caption=Scala Euclidean Distance]
def euclideanDistance(c1: List[Double], c2: List[Double]): Double = {
  val delta: Set[Double] = for {(a,b) <- c1 zip c2} yield math.pow(a-b, 2)
  Double = math.sqrt(delta.sum)
}
\end{lstlisting}

The reason why using KNN on n-dimensions is that you can classify different
types of data. A 2d KNN classifier could classify on two variables, such as
weight and height, while a 256d classifier could classify a 16x16 monochrome
image where each pixel is represented by a dimension.

\subsection{Computing the nearest neighbour}

Computing the nearest neighbour to a point is simply as easy as finding the point
in the training data that has the smallest euclidean distance to it, as shown in
Listing~\ref{nearestNeighbour}.

\begin{lstlisting}[label=nearestNeighbour, caption=Scala Nearest Neighbour]
def nearestNeighbour(input: Point, existing: Set[Point]): Point = {
  existing.min((p: Point) => p.euclideanDistance(input))
}
\end{lstlisting}

\marginpar{Assume that there is a custom implementation of {\tt Point} that
exposes the method to find the euclidean distance between it and another point,
similar to that in Listing~\ref{scala_nn}}

\subsection{Multiple nearest neighbours (K-NN)}

It's possible that more accurate classifications of points could be obtained by
taking into account multiple close neighbours rather than one nearest one. In
order to do this, you need to find the number of occurrences of a specific class
in the top $K$ nearest neighbours:

%TODO: Check this compiles

\begin{lstlisting}[label=nearestNeighbour, caption=Scala Nearest Neighbour]
def kNearestNeighbour(k: Int, input: Point, existing: Set[Point]): Class = {
  val topK: List[Class] = existing.toList.sorted(
      (p: Point) => p.euclideanDistance(input)
    ).take(k).map(_.class)
  topK.groupBy(identity).maxBy(_._2.size)._1
}
\end{lstlisting}

One must be careful when choosing a value of $K$ for the classifier. As $K$
increases proportional to the size of the dataset, then the number of incorrect
classifications will increase.

Figure~\ref{fig:knn-large} shows two identical datasets that a KNN
classification is being applied to. When $K = 3$, the correct classification of
a square is made, and when $K = 5$, the wrong classification is made, simply
because there are more circles than squares in the dataset.

\begin{figure}[!h]
  \includestandalone[width=\textwidth]{diagrams/knn-large}
  \caption{How K affects classification accuracy. The item being classified is
  the filled square and nearest neighbours are filled, and other elements are
  left unfilled. When K=3, it would be classified as square, when K=5, it'd be
  classified as a circle.}
  \label{fig:knn-large}
\end{figure}

With a nearest neighbour classifier, there is always one or more
\textit{boundaries} that defines which class a new item will be placed in, as
shown in Figure~\ref{fig:knn-boundary}. Be aware that the \textbf{Decision
Boundary} isn't always contiguous or a straight line.

\begin{figure}[!h]
  \centering
  \includestandalone[width=0.4\textwidth]{diagrams/knn-boundary}
  \caption{The dotted line represents the decision boundary for a KNN
  classifier}
  \label{fig:knn-boundary}
\end{figure}

\subsection{Overfitting}

Overfitting is a problem with all ML algorithms, and occurs when an algorithm is
trained on a small proportion of the dataset, that isn't Representative of the
whole data. It can also be when the algorithm is trained in such a way that it
describes the noise or random error in the training set.

Avoiding overfitting is hard, but generally it involves choosing the right
algorithm, the right parameters and the right training data.

\section{Linear Classifier}

The nearest neighbour algorithm requires a minimal amount of logic in the
training stage, since it's really only the value of $K$ that needs to be
considered. In fact, for a simple knn classifier, you don't even need to train
it, since you could just pick an arbitrary value for $K$, and then all the work
of the algorithm would be done at the `Model' stage? However, most other ML
algorithms require some kind of learning phrase before they can become useful.
The simplest of these is probably the linear classifier.

A simple linear classifier is one that looks at only one parameter, and can
classify into two classes. Essentially it says:

\begin{lstlisting}[label=linearClassifier, caption=A simple linear classifier]
if(parameter > threshold) class1 else class2
\end{lstlisting}

This classifier would work well for some limited use cases; maybe if we were
classifying boxers into different weight categories. However, we need to decide
on a threshold value in order for the classifier to work. In the case of boxing
this is easy, since there are known weight categories, however, in some other
cases, we might have to discover the threshold value from the data.

If there are two classes in the data which are linearly separable (i.e. there is
one or more threshold(s) that will accurately tell them apart), then an
algorithm to tell them apart is easy to write so long as you know the correct
threshold value. In order to find the right threshold value, you need a learning
algorithm such as this:

\begin{lstlisting}[label=linearClassifierLearningAlgorithm, caption=Linear classifier learning algorithm]
var errors: Int = 0
var threshold: Int = 0;
do {
  // classify will use a linear classifier to classify the data with a specific
  // threshold, and return the number of errors.
  errors = classify(data, correctLabels, threshold++)
} while(errors > 0);
\end{lstlisting}

In all learning algorithms, an error function is needed to evaluate whether the
changes to the parameters for the classifier on each iteration have had a
positive or negative effect on the accuracy of classification. The error
function in this case is \texttt{while(errors > 0)}, since we are assuming that
the data is linearly separable, we can just keep incrementing the threshold
value until we get a one that produces no errors.

\begin{figure}[!h]
  \includestandalone[width=\textwidth]{diagrams/generic-ML-algorithm}
  \caption{A generic ML algorithm always has the above structure.}
  \label{fig:generic-ML-algorithm}
\end{figure}

\section{Perceptron}

A peceptron is an ML algorithm that mimics a single neuron in a brain. Despite
emulating only one neuron, a peceptron can accurately classify a wide variety of
data.

The algorithm has $d$ inputs ($i_0, \dots, i_d$), and $d$ weights ($w_0, \dots,
w_d$), where each input has a specific weight associated with it. It also has
another parameter $t$, which is the classification threshold. The algorithm can
be expressed easily in mathematical notation:

\[
  classify(I) = 
  \begin{cases}
    1 & \text{if } t \leq \sum\limits^d_{j=0}i_jw_j\\
    0 & \text{otherwise}
  \end{cases}
\]

For the less mathematically inclined, here's the MATLAB code that does the same
thing:

\begin{lstlisting}[language=matlab, label=perceptron,
                   caption=A perceptron implementation in MATLAB]
  function output = perceptron(threshold, inputs, weights)

  % Compute the activiation level
  activation_level = sum(inputs.*weights);

  if activation_level > threshold
    output = 1;
  else
    output = 0;
\end{lstlisting}

Therefore:

\begin{verbatim}
  $ inputs = [0.1, 0.4, 0.2]
  $ weights = [0.9, 0.1, 0.7]
  $ threshold = 0.14
  $ perceptron(threshold, inputs, weights)
    > 1
  $ threshold = 0.32
  $ inputs = [0.1, 0.6, 0.1]
  $ perceptron(threshold, inputs, weights)
    > 0
\end{verbatim}

Training a perceptron is a matter of adjusting the weights and the threshold to
get the best classification accuracy on the training data. We need a rule that
we can apply over and over again to the parameters to refine them towards better
values:

\[
  weight = weight \times \text{\it learning\_rate} \times (actual - output) \times input
\]

Henceforth, we can create a learning algorithm for the perceptron. Lines 5 and 6
of Listing~\ref{peceptronTraining} are the implementation of the perceptron
learning rule, everything else is the logic to loop over all the weights a
certain number of times.

\begin{lstlisting}[language=java, label=perceptronTraining,
                   caption=A perceptron learning algorithm in Java]
  for(int i = 0; i < iterations; i++) {
    for(int example = 0; example < trainingSet.length; example++) {
      int output = perceptron(threshold, trainingSet[example], weights);
      for(int weight = 0; weight < weights.length; weight++) {
        int scaleFactor = learningRate * (trainingLabels[example] - output);
        weights[weight] = weights[weight] * trainingSet[example][weight] * learningRate;
      }
    }
  }
\end{lstlisting}

Note how computationally intensive the training algorithm is. The time
complexity is $O(i \cdot t \cdot w)$ where $i$ is the number of training
iterations to do, $t$ is the number of training examples, and $w$ is the number
of weights/inputs/dimensions to classify with.

Even though training is expensive, classification is relatively cheap, with a
linear runtime.

There is a theorem, called the \textbf{Perceptron Convergence Theorem}, that
states ``if the data is linearly separable, then application of the perceptron
learning rule will find a decision boundary within a finite number of
iterations''.

\subsection{Multilayer Perceptrons (MLP)}

A a multilayer perceptron is basically a graph structure, where every node is a
perceptron, and edges are connections from the output of one perceptron to the
input of another. In order to make this work, a different type of perceptron is
often used. Instead of having a threshold value that is compared to the sum of
the weighted inputs, the sum is run through a sigmoid funciton before being
output.

With a network of perceptrons, the decision boundary can now be curved and
doesn't have to be linear (as it is with only one perceptron). This means more
complex problems can be attempted.

In order to train a neural network, a technique called \textit{backpropogation}
is used. However, this is beyond the scope of this course.


\section{Decision trees}

% (Linear classifier == Decision stump) because it's like a decision tree with one level


