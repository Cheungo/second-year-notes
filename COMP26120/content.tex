% Set the author and title of the compiled pdf
\hypersetup{
  pdftitle = {\Title},
  pdfauthor = {\Author}
}

\input{listingstyle.tex}

\section{Algorithmic complexity and performance}

Algorithmic complexity and the big-oh notation allows us to characterise the
time and space requirements of an algorithm when it is given varying input data.
The big-oh notation allows us to get a good approximation of the upper and lower
bounds of an algorithm's complexity.

We can work out such an approximation by analysing (and generalising) the number
of logical operations an algorithm might do, rather than inspecting it's
performance in an implementation. This allows us to compare the merits of
different algorithms irrespective of their implementation.

The big-oh notation is shown below:

\[
  O(growth rate)
\]

The growth rate represents the rate at which the complexity of the algorithm
will change with the size of the input.

Growth rates that are either exponential or factorial in nature (or are perhaps
even worse than this) are said to be intractable \marginpar{Tractable
(\textit{Adjective})\\Easy to deal with.}, while algorithms with other
computational complexities are said to be tractable.

\newcommand\multibrace[3]{\rdelim\}{#1}{3mm}[\pbox{#2}{#3}]}

\begin{table}[h!]
  \centering
  \begin{tabularx}{0.75\textwidth}{>{$}l<{$} l l}
    \text{Complexity} & Growth rate \\ \cline{1-2}
    O(1)              & None\\
    O(log n)          & Logarithmic\\
    O(n^k)            & Polynomial\\
    O(n)              & Linear & \multibrace{3}{4.6cm}{
                                  All of these are special cases of polynomials,
                                  $n^1, n^2$ and $n^3$ respectively
                                } \\
    O(n^2)            & Quadratic\\
    O(n^3)            & Cubic\\
    O(k^n)            & Exponential\\
    O(n!)             & Factorial\\
  \end{tabularx}
  \caption{A number of common complexities and their equivalent growth rates}
  \label{table:complexity}
\end{table}

\subsection{Simplifying Big-Oh expressions}

In order to simplify the big-oh complexity of an algorithm you just isolate the
fastest growing term in the equation (i.e. whatever term comes furthest down in
Table~\ref{table:complexity}). You then remove all the constants from the
equation.

%TODO: Big Oh Example! Ohh no!

\subsection{Analysing algorithmic complexity}

There are two ways of finding the complexity of an algorithm, to inspect the
psudo code for it, or by implementing the algorithm and experimentally
determining the change in it's runtime with different input sizes.

\marginpar{Remember to check that your psudo code correctly implements the
algorithm before you try this.}

In order to analyse the psudo code to work out the complexity, you must look at
how many primitive operations it will use for different sizes of input.
Primitive operations are defined as memory accesses, arithmetic operations,
comparisons and the like.

As a general rule, loops, recursion and other constructs for repeatedly
performing operations will the best indicator as to the complexity of
algorithms.

If an algorithm is reducing the data it has to work with every so often, then it
may have a logarithmic runtime. For example, the algorithm utilises a binary
chop (such as binary search), then the runtime probably has a $log_2$ inside.
See section~\ref{subsubsec:logs} for a bit more on logs.

In order to determine complexity experimentally, you must implement the
algorithm in a programming language of your choice, then run the program for
different input sizes and measure the runtime and the memory used. Plot the data
on a graph and extrapolate as needed. From the curve of the graph, it is
possible to predict the complexity of the algorithm.

\subsubsection{A refresher on logs}
\label{subsubsec:logs}

A logarithm of a number is the exponent to which another number (the base) must
be raised to produce that number:

\begin{gather*}
  \forall b,x,y \in \mathbb{Z}\\
  y = b^x \Leftrightarrow x = log_b(y)
\end{gather*}

Henceforth, $2^4 = 16$ and $log_2(16) = 4$.

\subsubsection{Finding the maximum input size}

If we know the complexity and running space/time of an algorithm for a specific
implementation and input, we might want to know the input size we could run the
algorithm with for the specific machine in a specific time, or within a specific
space limit.

The way you do this is by solving the big-oh equation for $t$ instead of $n$.
For example, if the algorithm takes $30$ seconds to process $1000$ kilobytes of
data, how long will it take if we double the processing speed, given that the
algorithm runs in $O(n^3)$ time?

%TODO: Work out if this is BS or not?

\begin{gather*}
  t = n^3\\
  \sqrt[\leftroot{-0}\uproot{3}3]{t} = n\\
  \text{When we double $n$ and $t$:}\\
  2n = \sqrt[\leftroot{-0}\uproot{3}3]{2t}\\
  2n = 1.25992104989t\\
  30 / 1.25992104989 \approx 24\\
\end{gather*}

\subsection{The master theorem}

\marginpar{See pages 268-270 in the course textbook for more information}

The master method is a way of solving divide and conquer recurrence equations
without having to explicitly use induction. It is used when an algorithm's
complexity is of the form:

\[
  T(n) = 
  \begin{cases}
    c               & \text{if $n \le d$}\\
    aT(n/b) + f(n)  & \text{if $n \geq d$}
  \end{cases}
\]

Where $f(n)$ is a function that is positive when $n \geq d$ and:

\begin{tabular}{>{$}l<{$}}
  d \geq 1\\
  a > 0\\
  b > 1\\
  c > 0\\
  d \in \mathbb{Z}\\
  a,b,c \in \mathbb{R}
\end{tabular}

Such a recurrence relation occurs whenever an algorithm uses a divide and
conquer approach. Such an algorithm will split the problem into $a$ subproblems,
each of size $n/b$ before recursively solving them and merging the result back
together. In this case, $f(n)$ is the time it takes to split the problem into
subproblems and merge them back together after the solving is done.

The master theorem is defined by three cases:

\begin{enumerate}
  \item If there is a small constant $\epsilon > 0$ such that $f(n)$ is
        $O(n^{log_{b}{a-\epsilon}})$, then $T(n)$ is $\Theta(n^{log_ba})$

        An example could be when the recurrence relation is
        \[
          T(n) = 4T(n/2) + n
        \]
        since $n^{log_{b}{a}} = n^{log_{2}{4}} = n^2$, therefore $T(n) =
        \Theta(n^{log_24}) = \Theta(n^2)$.
  \item % TODO:
        Work out what's going on...
\end{enumerate}

\section{Algorithmic correctness}

\section{Data structures}

\section{Basic algorithms}

\subsection{Sorting}

A sorting algorithm takes as an input, an array of keys, where there is a
\textit{total order} on the keys (each key can be compared to another), and
produces as an output, the array where the keys are ordered according to their
order.

A total order is a relation that is transitive ($a \leq b \wedge b \leq c \implies a
\leq c$), anti-symmetric ($a \leq b \wedge b \leq a \implies a = b$), and
unsurprisingly, total ($a \leq b \wedge b \leq a$).

If the ordering on the elements of the array isn't a total order then bad
things can happen when you try and use an algorithm to sort the data. For
example, if you were to try and sort an array of rocks, papers and scissors
according to the rules of the traditional game, then you wouldn't have a
transitive relation, and a sorting algorithm would probably loop infinitely.

\subsection{The complexity of sorting}

The most common sorting algorithms are $O(n^2)$ time (this includes $O(n
\log{n})$ algorithms too). Even though a polynomial sort time is good, sometimes
we have billions of items to sort, and henceforth, a very long running time. In
this situation, $n \log{n}$ sorts are much more preferable to $n^2$ sorts.

Since we often don't know in advance exactly what data any sorting function will
be given in advance, its important to know both the upper and lower bounds on
the complexity of the sorting algorithm we're using.

\begin{description}
  \item The \textbf{upper bound} is the worst case time complexity of the
  sort. For example the worst case complexity of Merge Sort is $O(n \log{n})$.

  \item The \textbf{lower bound} complexity is always at least $O(n)$ for
  sorting algorithms, since every item needs to be looked at once (if only to
  check that the list is already sorted). Bucket, radix and bubble sort all
  achieve this for certain inputs. No comparison based sort can ever achieve
  a lower bound with less than $O(log_2(n!))$ comparisons.
\end{description}

\subsubsection{Worst case for comparison sorts}

If we were to draw a decision tree for each possible path of a comparison sort,
then we would get one with a depth of $log_2(n!)$, that produced all $n!$
permutations of the input. An asymptotically optimal comparison sort must travel
down this tree in its quest to find the answer. MergeSort and HeapSort are
optimal, while QuickSort is optimal for most inputs.

\subsection{Sorting algorithms in detail}
\begin{description}
\item \textbf{QuickSort} \\
  QuickSort is a \textbf{divide and conquer} algorithm, that uses a
  \textbf{comparison based} method to sort items. QuickSort can be implemented
  as an \textbf{in-place} sort, \textbf{stable}, though it is usually done so
  with recursion which gives it a space complexity of at least $O(log(n))$. This
  is a small inconvenience really though.

  First the list is partitioned into two halves. To do this, a random pivot is
  chosen from the list, and of the two new lists, one has the items less than
  the pivot, and the other has the items greater or equal to it.

  QuickSort is then applied to each sublist, so make them sorted, and then the
  start of the right list is joined to the end of the left list.

  See Listing~\ref{quicksort} for an example implementation.

\item \textbf{MergeSort} \\
  MergeSort is another \textbf{divide and conquer} algorithm that also uses a
  \textbf{comparison based} approach. MergeSort can \textbf{not be implemented
  in place}, but it is \textbf{stable}.

  It is similar to quicksort, except it's worst case run time is $O(n\log{n})$.
  First the list is split down the middle, and the two halves are sorted using
  merge sort recursively. Then, the two lists are merged back into one sorted
  list in $O(n)$ time. See Listing~\ref{mergesort} for an implementation.

\item \textbf{BucketSort} \\
  BucketSort is a \textbf{stable}, \textbf{distribution based} sorting
  algorithm. You place elements into `buckets' that describe a class of objects
  (you could order people by the first letter of their first name for example).
  When you've done that, you can either sort each bucket (using a different
  algorithm), then you simply return each bucket in order.

  BucketSort has a complexity of $O(n + k)$ where $k$ is the number of buckets
  you have. In the person name example, the number of buckets would be $26$,
  which, depending on the size of $n$ could be a very large, or very small
  factor.

\item \textbf{RadixSort}\\
  A RadixSort is basically bucket sort with multiple iterations. When you have
  sorted the first digit/word etc, you apply the same sort to each bucket in
  turn, until all of the buckets have size 1. RadixSort has a time complexity of
  $O(n)$

  RadixSort can be done \textbf{in-place}, and in a \textbf{stable} manner:
  \begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
    050, 731, 806, 014, 235
    05\textbf{0}, 73\textbf{1}, 01\textbf{4}, 23\textbf{5}, 80\textbf{6}
    8\textbf{0}6, 0\textbf{1}4, 7\textbf{3}1, 2\textbf{3}5, 0\textbf{5}0
    \textbf{0}14, \textbf{0}50, \textbf{2}35, \textbf{7}31, \textbf{8}06
  \end{Verbatim}

\item \textbf{HeapSort} \\
  HeapSort is very simple, it iterates through the unsorted list, adding each
  element to a heap as it goes. When it has finished adding all the elements, it
  simply iterates through the heap and returns the order of iteration. Since
  adding an element to a (binary) heap is an $O(log(n))$ operation, and you need
  to do it $n$ times, the runtime of HeapSort is $O(n\log{n})$.

  HeapSort can be implemented \textbf{in place} since the heap can be stored
  inside the input array (since a heap can be represented as an array). HeapSort
  isn't stable though, since the heap can be re-ordered while it's being created
  (if you're not sure on this, maybe it's time for a trip to wikipedia to learn
  about heaps).

\end{description}


\subsection{Searching}

\subsection{Tree and graph traversal}

\section{Code listings}

\lstinputlisting[caption={QuickSort}, label=quicksort]{code/QuickSort.java}
\lstinputlisting[caption={MergeSort}, label=mergesort]{code/MergeSort.java}