% Set the author and title of the compiled pdf
\hypersetup{
	pdftitle = {\Title},
	pdfauthor = {\Author}
}

\section{Distributed computing}
% Lecture One

A distributed system is a computing platform build with many computers that:

\begin{itemize}
  \item Operate concurrently,
  \item Are physically distributed (and can fail independently)
  \item Are linked by a network
  \item Have independent clocks
\end{itemize}

Leslie Lamport once said that:

\begin{aquote}{Leslie Lamport}
You know you have a distributed system when the crash of a computer youâ€™ve never
heard of stops you from getting any work done.
\end{aquote}

The consequences of having a distributed system is that many problems can arise
from a lack of synchronization and coordination between parts of the system.
Problems include:

\begin{itemize}
  \item Non-determinism
  \item Race conditions
  \item Deadlocks and synchronization
  \item No notion of a correct time (no global clock)
  \item No (visible) global state
  \item Parts of the system may fail independently
\end{itemize}

Despite these problems, we continue to (and increasingly commonly) build systems
and software designed to run on distributed hardware. This is for many reasons,
including the fact that people are distributed and move around a lot, information
needs to be shared, hardware can be shared to reduce costs or work in parallel
etc.

Distributed systems have evolved from simple systems in the 70's and 80's. Early
systems were for banks and airline booking systems, but the real proliferation
of the technique arose with the internet in the early 90's.

There are eight so called \textit{fallacies} of distributed computing:

\begin{enumerate}
  \item \textbf{The network is reliable}\\
    The network could stop working at any time for a variety of reasons;
    hardware failure, malicious actors etc. In order to protect against this, we
    need to build clever software that can resend failed messages, reorder
    messages, verify the integrity of messages etc.
  \item \textbf{Latency is zero}\\
    Latency is the time it takes from a message to get from one place to
    another. Even if the data is going at the speed of light, then a packet
    going from London to the east coast of the USA will take $30 milliseconds$.
    Developers should make as few calls to networked machines as possible, and
    transfer as much data as possible each time.
  \item \textbf{Bandwidth is infinite}\\
    Bandwidth is how much data you can send in a certain amount of time, and is
    measured in bits per second. Bandwidth is growing as technology improves,
    but so do the data requirements of applications, meaning that it is still an
    issue. Lost packets can reduce bandwidth, so increasing packet size can
    help. Compression can also be of use.
  \item \textbf{The network is secure}\\
    Since networks are largely insecure, you need to think about application
    security all the time. Implementing access control etc is a good idea for
    networked applications.
  \item \textbf{The network topology doesn't change}\\
    Since we don't control the network, servers could be added or removed,
    clients can change addresses etc and we won't know in advance. Distributed
    applications must be adaptive and work around these unexpected changes. The
    DNS system is a good example.
  \item \textbf{There is one administrator}\\
    Different people are in charge of different networks, even different parts 
    of networks. Diagnosing a problem may require the help (and cooperation) of
    multiple people and organisations.
  \item \textbf{Transport cost is zero}\\
    Not only do networks cost money (buying bandwidth, servers etc), but they
    also cost in terms of computing resources. Serialising between data formats
    and protocols takes lots of CPU cycles.
  \item \textbf{The network is homogeneous}\\
    Interoperability is required for heterogeneous systems to work together
    properly. Using standard technologies and data formats makes this easier
    (for example, returning data in JSON format from a REST API instead of a
    binary blob).
\end{enumerate}

% TODO: How to calculate latency
\section{Parallelising processes}
% Lecture 2

Many applications can be parallelised by doing homogeneous operations on different
processors on different data. If this is the case, in ideal conditions, your
speedup will be the same as the number of processors you're using as opposed to
using just one processor.

Unfortunately for us, the speedup is not linear, since it takes time to split
the data, coordinate the machines and collate the results. There is also a limit
to how many processors will keep the speed improving or even keeping constant.
If we have more processors than we can actually use, then the overhead of
managing them will probably decrease performance, since they'll be doing nothing
useful.

It is important to recognise that parallel computing is different to distributed
computing. Although they have similar goals and are achieved using similar
techniques, parallel computing is usually when you use multiple CPU's in the
same computer, whereas distributed computing is using networked computers.

You can still parallelise an application over different systems using the
network as a medium. Not all applications will benifit from this; the most
suitable applications have CPU intensive sections that don't require much
communication between nodes. If the proportion of your application that you can
speed up is $x$ (where $0 \leq x \geq 1$), then the maximum speedup you can
achieve is $\frac{1}{x}$.

The running time of a program executing on $n$ CPU's, when it runs in $t$
seconds on one cpu is:

\[
  \text{Running time} = overhead + t\left(1 - x _ \frac{x}{p}\right)
\]

Where $overhead$ is the time it takes to setup, synchronise and communicate
between CPU's. In pracrice, the $overhead$ is a function that takes the number
of processors as an argument (since it will usually increase as the number of
processors increases).

%TODO: Example of this maybe? There's one on page 10 of lecture 2

\subsection{Finding parallelisable portions of a program}

Instructions are well suited to parallel execution if they are either
independent of instructions around them (so the result of an instruction doesn't
change the result of another), or the same instructions are executed on multiple
data (such as mapping over an array).

Loops are a very good source of parallelism, since they are usually responsible
for repetitive operations on large amounts of data.

\section{Architectures of distributed systems}
% Lecture 3
